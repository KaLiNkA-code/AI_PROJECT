{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import datetime\n",
    "# import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "#  from nltk.corpus import stopwords\n",
    "#  from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# stop = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# ТАК НЕ ДЕЛАТЬ \n",
    "from preprocess import (\n",
    "    belongs_to_collection_func, \n",
    "    preprocess_genders, \n",
    "    preprocess_companies, \n",
    "    preprocess_languages,\n",
    "    preprocess_keywords, \n",
    "    preprocess_cast, \n",
    "    preprocess_crew,\n",
    "    preprocess_countries, \n",
    "    preprocess_homepage\n",
    "\n",
    ")\n",
    "\n",
    "import preprocess as pre\n",
    "\n",
    "# from preprocess import (\n",
    "#     preprocess_cast,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#  import plotly.offline as py\n",
    "#  py.init_notebook_mode(connected=True)\n",
    "#  import plotly.graph_objs as go\n",
    "#  import plotly.tools as tls\n",
    "#  import xgboost as xgb\n",
    "#  import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "#  import shap\n",
    "#  from catboost import CatBoostRegressor\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f2d5bc8da572783121324a1544483cc1dcaaa4d"
   },
   "source": [
    "<a id=\"data_loading\"></a>\n",
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "52ed3da69987f737ae87ffb99496ebc28a1203e6"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# from this kernel: https://www.kaggle.com/gravix/gradient-in-a-box\n",
    "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "\n",
    "def text_to_dict(df):\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "        \n",
    "train = text_to_dict(train)\n",
    "test = text_to_dict(test)\n",
    "\n",
    "train, test = belongs_to_collection_func(train, test)\n",
    "train, test, list_of_genres, top_genres = preprocess_genders(train, test)\n",
    "\n",
    "print('Number of production companies in films')\n",
    "train, test, list_of_companies, top_companies = preprocess_companies(train, test)\n",
    "\n",
    "train, test, list_of_countries, top_countries = preprocess_countries(train, test)\n",
    "\n",
    "train, test, list_of_languages, top_languages = preprocess_languages(train, test)\n",
    "\n",
    "train, test, list_of_keywords, top_keywords = preprocess_keywords(train, test)\n",
    "\n",
    "train, test, list_of_cast_names, list_of_cast_names_url, list_of_cast_genders, list_of_cast_characters, top_cast_names, top_cast_characters = preprocess_cast(train, test)\n",
    "\n",
    "train, test, list_of_crew_names_temp, list_of_crew_names, list_of_crew_names_url, list_of_crew_jobs, list_of_crew_genders, list_of_crew_departments, top_crew_names, top_crew_jobs = preprocess_crew(train, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, data_dict = preprocess_crew(train, test)\n",
    "# data_dict['list_of_crew_names_temp']\n",
    "\n",
    "train, test = preprocess_homepage(train, test)\n",
    "\n",
    "train['log_revenue'] = np.log1p(train['revenue'])\n",
    "train['log_budget'] = np.log1p(train['budget'])\n",
    "test['log_budget'] = np.log1p(test['budget'])\n",
    "\n",
    "# EDA - Explantory Data Analyses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['release_date'].isnull() == True, 'release_date'] = '01/01/98'\n",
    "\n",
    "def fix_date(x):\n",
    "    \"\"\"\n",
    "    Fixes dates which are in 20xx\n",
    "    \"\"\"\n",
    "    year = x.split('/')[2]\n",
    "    if int(year) <= 19:\n",
    "        return x[:-2] + '20' + year\n",
    "    else:\n",
    "        return x[:-2] + '19' + year\n",
    "\n",
    "train['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\n",
    "test['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\n",
    "train['release_date'] = pd.to_datetime(train['release_date'])\n",
    "test['release_date'] = pd.to_datetime(test['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features based on dates\n",
    "def process_date(df):\n",
    "    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n",
    "    for part in date_parts:\n",
    "        part_col = 'release_date' + \"_\" + part\n",
    "        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_date(train)\n",
    "test = process_date(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\n",
    "test = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].nunique() == 1:\n",
    "        print(col)\n",
    "        train = train.drop([col], axis=1)\n",
    "        test = test.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ba0024111403d77d8ab177767e0ddd5a40455dd"
   },
   "source": [
    "<a id=\"basic_model\"></a>\n",
    "## Modelling and feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4ceed1b74dd3de3b7678afa5409fc408464d8f0"
   },
   "outputs": [],
   "source": [
    "for col in ['original_language', 'collection_name', 'all_genres']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n",
    "    train[col] = le.transform(train[col].fillna('').astype(str))\n",
    "    test[col] = le.transform(test[col].fillna('').astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c0104e927d1ad9f0c23cc5d12d04b34c0893325"
   },
   "outputs": [],
   "source": [
    "train_texts = train[['title', 'tagline', 'overview', 'original_title']]\n",
    "test_texts = test[['title', 'tagline', 'overview', 'original_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f05aa3c34b2b1d3ca528461e2c655545773b5245"
   },
   "outputs": [],
   "source": [
    "for col in ['title', 'tagline', 'overview', 'original_title']:\n",
    "    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n",
    "    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n",
    "    train = train.drop(col, axis=1)\n",
    "    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n",
    "    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n",
    "    test = test.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "55356a620f90a4475c41eaedbd95db6f83543725"
   },
   "outputs": [],
   "source": [
    "# data fixes from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n",
    "train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\n",
    "train.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \n",
    "train.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\n",
    "train.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\n",
    "train.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \n",
    "train.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\n",
    "train.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\n",
    "train.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\n",
    "train.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n",
    "train.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\n",
    "train.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\n",
    "train.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \n",
    "train.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \n",
    "train.loc[train['id'] == 1542,'budget'] = 1              # All at Once\n",
    "train.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\n",
    "train.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\n",
    "train.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\n",
    "train.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\n",
    "train.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n",
    "train.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\n",
    "train.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\n",
    "train.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\n",
    "train.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\n",
    "train.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\n",
    "test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\n",
    "test.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\n",
    "test.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\n",
    "test.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\n",
    "test.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\n",
    "test.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\n",
    "test.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\n",
    "test.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\n",
    "test.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\n",
    "test.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n",
    "\n",
    "power_six = train.id[train.budget > 1000][train.revenue < 100]\n",
    "\n",
    "for k in power_six :\n",
    "    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d4f4a5b8945723acb5a1206cffc60fbab151ae8"
   },
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'revenue'], axis=1)\n",
    "y = np.log1p(train['revenue'])\n",
    "# y = train['revenue']\n",
    "X_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd747f7dc0506664d8dbe8e3eb4dc8b6c71f244a"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)  # , seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5d7d29134a25355c517250b00079dae64523f5e"
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ff384168227fc22728aea371bb5935d0f39ce6eb"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X, \n",
    "    X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "    \n",
    "    # out of fold - 1000\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    # \n",
    "    prediction = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "#         if model_type == 'sklearn':\n",
    "#             X_train, X_valid = X[train_index], X[valid_index]\n",
    "#         else:\n",
    "        X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "            \n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "    \n",
    "        if model_type == 'sklearn':\n",
    "            # Callable объект .fit\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_squared_error(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,) # Shape [1, 200] -> [200]  -1  всё остальное\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "    print(len(X_train), len(X_valid))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False -> 0 True -> 1\n",
    "\n",
    "\n",
    "from sre_compile import isstring\n",
    "\n",
    "X = X.dropna()\n",
    "X_test = X.dropna()\n",
    "#  X = X[np.isfinite (X)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 10,\n",
    "         'objective': 'regression',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "\n",
    "oof_linear, prediction_linear = train_model(\n",
    "    X, X_test, y, params=params, model_type='sklearn',\n",
    "    model=LinearRegression(),\n",
    "    plot_feature_importance=True\n",
    ")\n",
    "\n",
    "prediction_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e3f87bf30dbd425de5909fa2a8de6ee3cb6246"
   },
   "source": [
    "<a id=\"add_feat\"></a>\n",
    "### Additional feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33a897ca5503e3bb4b6ab28e89a4056644f116e1"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "526607c3368a9128f137931b68c59d0923006b1e"
   },
   "outputs": [],
   "source": [
    "def new_features(df):\n",
    "    df['budget_to_popularity'] = df['budget'] / df['popularity']\n",
    "    df['budget_to_runtime'] = df['budget'] / df['runtime']\n",
    "    \n",
    "    # some features from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n",
    "    df['_budget_year_ratio'] = df['budget'] / (df['release_date_year'] * df['release_date_year'])\n",
    "    df['_releaseYear_popularity_ratio'] = df['release_date_year'] / df['popularity']\n",
    "    df['_releaseYear_popularity_ratio2'] = df['popularity'] / df['release_date_year']\n",
    "    \n",
    "    df['runtime_to_mean_year'] = df['runtime'] / df.groupby(\"release_date_year\")[\"runtime\"].transform('mean')\n",
    "    df['popularity_to_mean_year'] = df['popularity'] / df.groupby(\"release_date_year\")[\"popularity\"].transform('mean')\n",
    "    df['budget_to_mean_year'] = df['budget'] / df.groupby(\"release_date_year\")[\"budget\"].transform('mean')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe3bc388686ec280b6a8b0f59a50afe009623d95"
   },
   "outputs": [],
   "source": [
    "X = new_features(X)\n",
    "X_test = new_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc6899e5988a42a0c64a29385868abe0510b9632"
   },
   "outputs": [],
   "source": [
    "oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6684ca81fe35bc8a1d12622b2f6686cf01d004bd"
   },
   "source": [
    "<a id=\"imp_feats\"></a>\n",
    "### Important features\n",
    "\n",
    "Let's have a look at important features using ELI5 and SHAP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff82b9193ff22a66f2d869ea11ad2d041c785084"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 6,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "model1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "model1.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "        verbose=1000, early_stopping_rounds=200)\n",
    "\n",
    "eli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6c8c94a775c4d7215bd442dfeeb8571ad464c67"
   },
   "source": [
    "We can see that important features native to LGB and top features in ELI5 are mostly similar. This means that our model is quite good at working with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ed18e95e2da01e8db6bbd9e6694626f5af46d8f"
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model1, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f09d214bb08b6f4e9ba4454c7a25b6450493fc1a"
   },
   "source": [
    "SHAP provides more detailed information even if it may be more difficult to understand.\n",
    "\n",
    "For example low budget has negavite impact on revenue, while high values usually tend to have higher revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb1a6744d4228f438d6c868ae667e3ee6a62ee97"
   },
   "outputs": [],
   "source": [
    "top_cols = X_train.columns[np.argsort(shap_values.std(0))[::-1]][:10]\n",
    "for col in top_cols:\n",
    "    shap.dependence_plot(col, shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c712b24954c7b0b86b16dfb91506b5b78f8db1b0"
   },
   "source": [
    "Here we can see interactions between important features. There are some interesting things here. For example relationship between release_date_year and log_budget. Up to ~1990 low budget films brought higher revenues, but after 2000 year high budgets tended to be correlated with higher revenues. And in genereal the effect of budget diminished.\n",
    "\n",
    "Let's create new features as interactions between top important features. Some of them make little sense, but maybe they could improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7aa37af7272d4403f238b78eed028d4f558fc806"
   },
   "outputs": [],
   "source": [
    "def top_cols_interaction(df):\n",
    "    df['budget_to_year'] = df['budget'] / df['release_date_year']\n",
    "    df['budget_to_mean_year_to_year'] = df['budget_to_mean_year'] / df['release_date_year']\n",
    "    df['popularity_to_mean_year_to_log_budget'] = df['popularity_to_mean_year'] / df['log_budget']\n",
    "    df['year_to_log_budget'] = df['release_date_year'] / df['log_budget']\n",
    "    df['budget_to_runtime_to_year'] = df['budget_to_runtime'] / df['release_date_year']\n",
    "    df['genders_1_cast_to_log_budget'] = df['genders_1_cast'] / df['log_budget']\n",
    "    df['all_genres_to_popularity_to_mean_year'] = df['all_genres'] / df['popularity_to_mean_year']\n",
    "    df['genders_2_crew_to_budget_to_mean_year'] = df['genders_2_crew'] / df['budget_to_mean_year']\n",
    "    df['overview_oof_to_genders_2_crew'] = df['overview_oof'] / df['genders_2_crew']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1dcc3499f4cfbc90d1a896011546b562d0adee75"
   },
   "outputs": [],
   "source": [
    "X = top_cols_interaction(X)\n",
    "X_test = top_cols_interaction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "381d4d7c8f7e439511ce3f7b84014dd417aaa57d"
   },
   "outputs": [],
   "source": [
    "X = X.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], 0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04073ee9ec7ce9667c8e93205c02bf85a6151b9c"
   },
   "source": [
    "<a id=\"ext_feats\"></a>\n",
    "### External features\n",
    "I'm adding external features from this kernel: https://www.kaggle.com/kamalchhirang/eda-feature-engineering-lgb-xgb-cat by kamalchhirang. All credit for these features goes to him and his kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9b40fa7274e33bef1318a5519b386586a120d0b"
   },
   "outputs": [],
   "source": [
    "trainAdditionalFeatures = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\n",
    "testAdditionalFeatures = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\n",
    "\n",
    "train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n",
    "test = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\n",
    "X['imdb_id'] = train['imdb_id']\n",
    "X_test['imdb_id'] = test['imdb_id']\n",
    "del train, test\n",
    "\n",
    "X = pd.merge(X, trainAdditionalFeatures, how='left', on=['imdb_id'])\n",
    "X_test = pd.merge(X_test, testAdditionalFeatures, how='left', on=['imdb_id'])\n",
    "\n",
    "X = X.drop(['imdb_id'], axis=1)\n",
    "X_test = X_test.drop(['imdb_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27f2315c67c8065f388f0955a1987637f4ff888a"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0125f421a8cc732156c60d8a1e13efbcbcc3d577"
   },
   "outputs": [],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 9,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1658d1b96c2ee02789f822f31a25425b727b0ef2"
   },
   "source": [
    "<a id=\"stacking\"></a>\n",
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f16996349c42b28229b13f33b974755b6202baf"
   },
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof_lgb, oof_xgb, oof_cat, oof_lgb_1, oof_lgb_2]).transpose()\n",
    "train_stack = pd.DataFrame(train_stack, columns=['lgb', 'xgb', 'cat', 'lgb_1', 'lgb_2'])\n",
    "test_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_cat, prediction_lgb_1, prediction_lgb_2]).transpose()\n",
    "test_stack = pd.DataFrame(test_stack, columns=['lgb', 'xgb', 'cat', 'lgb_1', 'lgb_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b93fb33cd8d417349678db6f2def72e2df6cb35c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91c27020935e11d96b41e395a535eb86a65837d2"
   },
   "outputs": [],
   "source": [
    "model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n",
    "oof_rcv_stack, prediction_rcv_stack = train_model(train_stack.values, test_stack.values, y, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29fddbb3e3e22345439c0edcf5ee53fd877168f6"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')\n",
    "sub['revenue'] = np.expm1(prediction_lgb)\n",
    "sub.to_csv(\"lgb.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) / 2)\n",
    "sub.to_csv(\"blend.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat) / 3)\n",
    "sub.to_csv(\"blend1.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat + prediction_lgb_1) / 4)\n",
    "sub.to_csv(\"blend2.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat + prediction_lgb_1 + prediction_lgb_2) / 5)\n",
    "sub.to_csv(\"blend3.csv\", index=False)\n",
    "\n",
    "sub['revenue'] = prediction_lgb_stack\n",
    "sub.to_csv(\"stack_lgb.csv\", index=False)\n",
    "sub['revenue'] = prediction_rcv_stack\n",
    "sub.to_csv(\"stack_rcv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "723d4b7bc280cd31fdada53ad6420192b9a3a8d60631096143cc718cb9440dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
