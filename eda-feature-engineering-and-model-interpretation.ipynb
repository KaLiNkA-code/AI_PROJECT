{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'm working with data from TMDB Box Office Prediction Challenge. Film industry is booming, the revunues are growing, so we have a lot of data about films. Can we build models, which will be able to accurately predict film revenues? Could this models be used to make some changes in movies to increase their revenues even further? I'll try answer this questions in my kernel!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "* [1 Data loading and overview](#data_loading)\n",
    "* [1.1 belongs_to_collection](#btoc)\n",
    "* [1.2 genres](#genres)\n",
    "* [1.3 Production companies](#production_companies)\n",
    "* [1.4 Production countries](#production_countries)\n",
    "* [1.5 Spoken languages](#lang)\n",
    "* [1.6 Keywords](#keywords)\n",
    "* [1.7 Cast](#cast)\n",
    "* [1.8 Crew](#crew)\n",
    "* [2 Data exploration](#de)\n",
    "* [2.1 Target](#target)\n",
    "* [2.2 Budget](#budget)\n",
    "* [2.3 Homepage](#homepage)\n",
    "* [2.4 Original language](#or_lang)\n",
    "* [2.5 Original title](#or_title)\n",
    "* [2.6 Overview](#overview)\n",
    "* [2.7 Popularity](#popularity)\n",
    "* [2.8 Release date](#release_date)\n",
    "* [2.9 Runtime](#runtime)\n",
    "* [2.10 Status](#status)\n",
    "* [2.11 Tagline](#tagline)\n",
    "* [2.12 Collections](#collections)\n",
    "* [2.13 Genres](#genres_)\n",
    "* [2.14 Production companies](#prod_comp)\n",
    "* [2.15 Production countries](#prod_count)\n",
    "* [2.16 Cast](#cast_viz)\n",
    "* [2.17 Keywords](#key_viz)\n",
    "* [2.18 Crew](#crew_viz)\n",
    "* [3 Modelling and feature generation](#basic_model)\n",
    "* [3.1 OOF features based on texts](#oof)\n",
    "* [3.2 Additional feature generation](#add_feat)\n",
    "* [3.3 Important features](#imp_feats)\n",
    "* [3.4 External features](#ext_feats)\n",
    "* [3.5 Blending](#blending)\n",
    "* [3.6 Stacking](#stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 666 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting eli5\n",
      "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 735 kB/s eta 0:00:01     |████████████████████████████▉   | 194 kB 735 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>17.1.0 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from eli5) (21.2.0)\n",
      "Collecting jinja2>=3.0.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 623 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from eli5) (1.19.5)\n",
      "Requirement already satisfied: scipy in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from eli5) (1.6.3)\n",
      "Requirement already satisfied: six in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from eli5) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from eli5) (0.24.2)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from jinja2>=3.0.0->eli5) (2.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from scikit-learn>=0.20->eli5) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from seaborn) (1.2.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from seaborn) (3.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (8.4.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/zakirov/miniconda3/envs/ml/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2021.1)\n",
      "Building wheels for collected packages: eli5\n",
      "  Building wheel for eli5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107729 sha256=8b9359085f1bd88e6121ac6e2a22246fcb0f92b5e6c86b9acc6c17940e6380df\n",
      "  Stored in directory: /Users/zakirov/Library/Caches/pip/wheels/7b/26/a5/8460416695a992a2966b41caa5338e5e7fcea98c9d032d055c\n",
      "Successfully built eli5\n",
      "Installing collected packages: tabulate, jinja2, graphviz, seaborn, eli5\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "Successfully installed eli5-0.13.0 graphviz-0.20.1 jinja2-3.1.2 seaborn-0.12.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import datetime\n",
    "# import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "#  from nltk.corpus import stopwords\n",
    "#  from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# stop = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# ТАК НЕ ДЕЛАТЬ \n",
    "from preprocess import *\n",
    "\n",
    "import preprocess as pre\n",
    "\n",
    "# from preprocess import (\n",
    "#     preprocess_cast,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#  import plotly.offline as py\n",
    "#  py.init_notebook_mode(connected=True)\n",
    "#  import plotly.graph_objs as go\n",
    "#  import plotly.tools as tls\n",
    "#  import xgboost as xgb\n",
    "#  import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "#  import shap\n",
    "#  from catboost import CatBoostRegressor\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f2d5bc8da572783121324a1544483cc1dcaaa4d"
   },
   "source": [
    "<a id=\"data_loading\"></a>\n",
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "52ed3da69987f737ae87ffb99496ebc28a1203e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of production companies in films\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# from this kernel: https://www.kaggle.com/gravix/gradient-in-a-box\n",
    "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "\n",
    "def text_to_dict(df):\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "        \n",
    "train = text_to_dict(train)\n",
    "test = text_to_dict(test)\n",
    "\n",
    "train, test = belongs_to_collection_func(train, test)\n",
    "train, test, list_of_genres, top_genres = preprocess_genders(train, test)\n",
    "\n",
    "print('Number of production companies in films')\n",
    "train, test, list_of_companies, top_companies = preprocess_companies(train, test)\n",
    "\n",
    "train, test, list_of_countries, top_countries = preprocess_countries(train, test)\n",
    "\n",
    "train, test, list_of_languages, top_languages = preprocess_languages(train, test)\n",
    "\n",
    "train, test, list_of_keywords, top_keywords = preprocess_keywords(train, test)\n",
    "\n",
    "train, test, list_of_cast_names, list_of_cast_names_url, list_of_cast_genders, list_of_cast_characters, top_cast_names, top_cast_characters = preprocess_cast(train, test)\n",
    "\n",
    "train, test, list_of_crew_names_temp, list_of_crew_names, list_of_crew_names_url, list_of_crew_jobs, list_of_crew_genders, list_of_crew_departments, top_crew_names, top_crew_jobs = preprocess_crew(train, test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, data_dict = preprocess_crew(train, test)\n",
    "# data_dict['list_of_crew_names_temp']\n",
    "\n",
    "train, test = preprocess_homepage(train, test)\n",
    "\n",
    "train['log_revenue'] = np.log1p(train['revenue'])\n",
    "train['log_budget'] = np.log1p(train['budget'])\n",
    "test['log_budget'] = np.log1p(test['budget'])\n",
    "\n",
    "# EDA - Explantory Data Analyses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['release_date'].isnull() == True, 'release_date'] = '01/01/98'\n",
    "\n",
    "def fix_date(x):\n",
    "    \"\"\"\n",
    "    Fixes dates which are in 20xx\n",
    "    \"\"\"\n",
    "    year = x.split('/')[2]\n",
    "    if int(year) <= 19:\n",
    "        return x[:-2] + '20' + year\n",
    "    else:\n",
    "        return x[:-2] + '19' + year\n",
    "\n",
    "train['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\n",
    "test['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\n",
    "train['release_date'] = pd.to_datetime(train['release_date'])\n",
    "test['release_date'] = pd.to_datetime(test['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-c14b5b450358>:6: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-c14b5b450358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-c14b5b450358>\u001b[0m in \u001b[0;36mprocess_date\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_parts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpart_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'release_date'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5459\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5460\u001b[0m         ):\n\u001b[0;32m-> 5461\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .dt accessor with datetimelike values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# creating features based on dates\n",
    "def process_date(df):\n",
    "    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n",
    "    for part in date_parts:\n",
    "        part_col = 'release_date' + \"_\" + part\n",
    "        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_date(train)\n",
    "test = process_date(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language_\n",
      "cast_character_\n"
     ]
    }
   ],
   "source": [
    "train = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\n",
    "test = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].nunique() == 1:\n",
    "        print(col)\n",
    "        train = train.drop([col], axis=1)\n",
    "        test = test.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'[1, 2, 3]' -> [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "343a6e043f1eb0120d44d0f3047b0d3b5c42103e"
   },
   "source": [
    "Drama, Comedy and Thriller are popular genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "905d5a3280380cae6a5daf0fc0bab267673d138c"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_genres for i in j]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "5e3a3bf02b6e91ec7a5db3849b09fe6e24aeece0"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_companies for i in j]).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "29ae28491ecbc18ec6e797dbd44220aa9fbba0c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of production countries in films\n"
     ]
    }
   ],
   "source": [
    "# print('Number of production countries in films')\n",
    "# Counter([i for j in list_of_countries for i in j]).most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "aae12a93c599bd2db0fd83e173400f4eb25dbf3c"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_languages for i in j]).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "29ae28491ecbc18ec6e797dbd44220aa9fbba0c0"
   },
   "outputs": [],
   "source": [
    "# print('Number of Keywords in films')\n",
    "# list_of_keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1c43d54e0d42b22fe8680afff86a6ed4bd8fc83"
   },
   "source": [
    "Here we have some keywords describing films. Of course there can be a lot of them. Let's have a look at the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a2095c17ef66dbf8dfebbc5cb86d858c2bca166f"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (16, 12))\n",
    "# text = ' '.join(['_'.join(i.split(' ')) for j in list_of_keywords for i in j])\n",
    "# wordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n",
    "#                       width=1200, height=1000).generate(text)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.title('Top keywords')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b08f2188f238277f5b60d807464c2c3c9d3d795"
   },
   "source": [
    "<a id=\"cast\"></a>\n",
    "### cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "b19b66732f300cb44cc714d96fa41f4eed533a44"
   },
   "outputs": [],
   "source": [
    "# for i, e in enumerate(train['cast'][:1]):\n",
    "#     print(i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "dbd0dec8d617c61e570b29acf34f4e2772779789"
   },
   "outputs": [],
   "source": [
    "# print('Number of casted persons in films')\n",
    "# list_of_cast_genders[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce756e03a96a57ae6fbe9bf873b3fa8c33087df9"
   },
   "source": [
    "Those who are casted heavily impact the quality of the film. We have not only the name of the actor, but also the gender and character name/type.\n",
    "\n",
    "At first let's have a look at the popular names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "0521284dd1e9306e4216d8b1f76eff390bdfc25a"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_cast_names for i in j]).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1ed74d753c746919df8ba141ed9d0ca17a2700d4"
   },
   "outputs": [],
   "source": [
    "# d = Counter([i for j in list_of_cast_names_url for i in j]).most_common(16)\n",
    "# fig = plt.figure(figsize=(20, 12))\n",
    "# for i, p in enumerate([j[0] for j in d]):\n",
    "#     ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "#     im = Image.open(urlopen(f\"https://image.tmdb.org/t/p/w600_and_h900_bestv2{p[1]}\"))\n",
    "#     plt.imshow(im)\n",
    "#     ax.set_title(f'{p[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "909c445e7a366209dc02ff0b0845f1787ce8f30e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 27949), (0, 20329), (1, 13533)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter([i for j in list_of_cast_genders for i in j]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee47bb92993915fdecb2b43df117154dcb66a93d"
   },
   "source": [
    "0 is unspecified, 1 is female, and 2 is male. (https://www.kaggle.com/c/tmdb-box-office-prediction/discussion/80983#475572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "8bf43638cd5fc4586a11486742a7c63b6145aab8"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_cast_characters for i in j]).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b63fb6070508dceae48dfdf5b0c6f549e4fe8ca"
   },
   "source": [
    "I think it is quite funny the most popular male role is playing himself. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e1bcfd996db24c9fbf560f5f872fcc829eb539"
   },
   "source": [
    "<a id=\"crew\"></a>\n",
    "### crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "94328af320a6794fe5ee62d9dce7fb03b64862ff"
   },
   "outputs": [],
   "source": [
    "# for i, e in enumerate(train['crew'][:1]):\n",
    "#     print(i, e[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "800aad5c8e02dcca35b54b5c967a40ca4990c7a5"
   },
   "outputs": [],
   "source": [
    "# print('Number of casted persons in films')\n",
    "# list_of_crew_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83e387c4db659a414c09ae60e26cd8b3172ffde6"
   },
   "source": [
    "The great crew is very important in creating the film. We have not only the names of the crew members, but also the genders, jobs and departments.\n",
    "\n",
    "At first let's have a look at the popular names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "573be403a4c380cabd2ddc814a82ce3cccb547f8"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_crew_names for i in j]).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b6524bced31b224291d98a348bf90b5120c416a6"
   },
   "outputs": [],
   "source": [
    "# d = Counter([i for j in list_of_crew_names_url for i in j]).most_common(16)\n",
    "# fig = plt.figure(figsize=(20, 16))\n",
    "# for i, p in enumerate([j[0] for j in d]):\n",
    "#     ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "#     if p[1]:\n",
    "#         im = Image.open(urlopen(f\"https://image.tmdb.org/t/p/w600_and_h900_bestv2{p[1]}\"))\n",
    "#     else:\n",
    "#         im = Image.new('RGB', (5, 5))\n",
    "#     plt.imshow(im)\n",
    "#     ax.set_title(f'Name: {p[0]} \\n Job: {p[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "dd65d86ad5738415e7d83719e28a05f7a3704331"
   },
   "outputs": [],
   "source": [
    "# Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64eb5d6d8c7d40e836288e80a3f768e3035dcf49"
   },
   "source": [
    "<a id=\"de\"></a>\n",
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "cff4344b90fb2bac25e55cc14172b30d05d7b288"
   },
   "outputs": [],
   "source": [
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3b1ff430c405f86a7ee93b1b69157bb084ab0f7"
   },
   "source": [
    "<a id=\"target\"></a>\n",
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "5cf4f7c462ad5cfac4a963e60609c88302647908"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize = (16, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(train['revenue']);\n",
    "# plt.title('Distribution of revenue');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(np.log1p(train['revenue']));\n",
    "# plt.title('Distribution of log of revenue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "_uuid": "1ee17e80484d16b0f1a98698804ca4cfc89fd7d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       16.326300\n",
       "1       18.370959\n",
       "2       16.387512\n",
       "3       16.588099\n",
       "4       15.182615\n",
       "          ...    \n",
       "2995    14.283442\n",
       "2996    12.103990\n",
       "2997    18.309266\n",
       "2998    18.962792\n",
       "2999    18.223292\n",
       "Name: log_revenue, Length: 3000, dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['log_revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55f380cbb5e97236d0f12e8fa87bf8161bcf9a9d"
   },
   "source": [
    "As we can see revenue distribution has a high skewness! It is better to use `np.log1p` of revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d20fa8b3f60ae3a0dec6330d2a34f7556bc51d2"
   },
   "source": [
    "<a id=\"budget\"></a>\n",
    "### Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a8e9e360c5197f8fb6d1dcd60b18404072ac65a3"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize = (16, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(train['budget']);\n",
    "# plt.title('Distribution of budget');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(np.log1p(train['budget']));\n",
    "# plt.title('Distribution of log of budget');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c31380dc4d5af5585194fa2a105777ffbbee1566"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(train['budget'], train['revenue'])\n",
    "# plt.title('Revenue vs budget');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(np.log1p(train['budget']), train['log_revenue'])\n",
    "# plt.title('Log Revenue vs log budget');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f612adfc76b16902e55592a65ffea6ab3e78f5fe"
   },
   "source": [
    "We can see that budget and revenue are somewhat correlated. Logarithm transformation makes budget distribution more managable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d07979f48f654584bc711166abf9b8f7e9af1abf"
   },
   "source": [
    "<a id=\"homepage\"></a>\n",
    "### homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "ca7c82027177c1688147e5cdafca34058d3a10d6"
   },
   "outputs": [],
   "source": [
    "# train['homepage'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5d8ab6893f3593c1ad25451cbf29ecc726a70fc"
   },
   "source": [
    "Most of homepages are unique, so this feature may be useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "_uuid": "9613b635b2dbaa0aa694d68ceb5bffaf9852c10d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "48c465c4ddc5eff187c21eaa60daa0dd72524289"
   },
   "outputs": [],
   "source": [
    "# sns.catplot(x='has_homepage', y='revenue', data=train);\n",
    "# plt.title('Revenue for film with and without homepage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b9cf5ac156fd383220cc97fdd302f85016d525c"
   },
   "source": [
    "<a id=\"or_lang\"></a>\n",
    "### original_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3ff01125c13c9d924380144c15368c837cc7fb36"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.boxplot(x='original_language', y='revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\n",
    "# plt.title('Mean revenue per language');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.boxplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\n",
    "# plt.title('Mean log revenue per language');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4ae0b8a930c4e3adb230ae3aef1a05058b636e3"
   },
   "source": [
    "<a id=\"or_title\"></a>\n",
    "### original_title\n",
    "\n",
    "It can be interesting to see which words are common in titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "26bf2e88857defd192524697aceb7a9dd983d045"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (12, 12))\n",
    "# text = ' '.join(train['original_title'].values)\n",
    "# wordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.title('Top words in titles')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc372a3bbc52ab93648f9223b6fae0dbc178c565"
   },
   "source": [
    "<a id=\"overview\"></a>\n",
    "### overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "72cdad64f13403a6526c781b32fa968872691c11"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (12, 12))\n",
    "# text = ' '.join(train['overview'].fillna('').values)\n",
    "# wordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.title('Top words in overview')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "735b9ce1ae41966d2d32b8810864aebf9524fb8e"
   },
   "source": [
    "Let's try to see which words have high impact on the revenue. I'll build a simple model and use ELI5 for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "b490f032442d1a80d615f01b570ea9edd3af56c0"
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(\n",
    "#             sublinear_tf=True,\n",
    "#             analyzer='word',\n",
    "#             token_pattern=r'\\w{1,}',\n",
    "#             ngram_range=(1, 2),\n",
    "#             min_df=5)\n",
    "\n",
    "# overview_text = vectorizer.fit_transform(train['overview'].fillna(''))\n",
    "\n",
    "\n",
    "# linreg = LinearRegression()\n",
    "# linreg.fit(overview_text, train['log_revenue'])\n",
    "# eli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "c7d6baca7d8e32b9200eaa01dbf9f1d282212a2f"
   },
   "outputs": [],
   "source": [
    "# print('Target value:', train['log_revenue'][1000])\n",
    "# eli5.show_prediction(linreg, doc=train['overview'].values[1000], vec=vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f89c14cce1ddf466363dd6cb919c089c29e65589"
   },
   "source": [
    "We can see that some words can be used to predict revenue, but we will need more that overview text to build a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "591fa9fc7b5c0586853b4fb50732bc94fe7b8691"
   },
   "source": [
    "<a id=\"popularity\"></a>\n",
    "### popularity\n",
    "\n",
    "I'm not exactly sure what does popularity represents. Maybe it is some king of weighted rating, maybe something else. It seems it has low correlation with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "18d70ea38888445c836b87cbcb8e70c2a141e9dd"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(train['popularity'], train['revenue'])\n",
    "# plt.title('Revenue vs popularity');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(train['popularity'], train['log_revenue'])\n",
    "# plt.title('Log Revenue vs popularity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca4363b01b1622407be10fc12df6f62a10416eb5"
   },
   "source": [
    "<a id=\"release_data\"></a>\n",
    "### release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "3186bbfe31e76dc5a5d295569de8091825b47c8f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0bff1942cc37fe110ee2e84b0541147621e5ab5b"
   },
   "outputs": [],
   "source": [
    "# d1 = train['release_date_year'].value_counts().sort_index()\n",
    "# d2 = test['release_date_year'].value_counts().sort_index()\n",
    "# data = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\n",
    "# layout = go.Layout(dict(title = \"Number of films per year\",\n",
    "#                   xaxis = dict(title = 'Year'),\n",
    "#                   yaxis = dict(title = 'Count'),\n",
    "#                   ),legend=dict(\n",
    "#                 orientation=\"v\"))\n",
    "# py.iplot(dict(data=data, layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ec64b016578e1e46257edf2d9c82a50c4b0e6c48"
   },
   "outputs": [],
   "source": [
    "# d1 = train['release_date_year'].value_counts().sort_index()\n",
    "# d2 = train.groupby(['release_date_year'])['revenue'].sum()\n",
    "# data = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\n",
    "# layout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n",
    "#                   xaxis = dict(title = 'Year'),\n",
    "#                   yaxis = dict(title = 'Count'),\n",
    "#                   yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n",
    "#                   ),legend=dict(\n",
    "#                 orientation=\"v\"))\n",
    "# py.iplot(dict(data=data, layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "03bc2c7ca73b51a16144bb57b869d3bf3c4c1a90"
   },
   "outputs": [],
   "source": [
    "# d1 = train['release_date_year'].value_counts().sort_index()\n",
    "# d2 = train.groupby(['release_date_year'])['revenue'].mean()\n",
    "# data = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\n",
    "# layout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n",
    "#                   xaxis = dict(title = 'Year'),\n",
    "#                   yaxis = dict(title = 'Count'),\n",
    "#                   yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n",
    "#                   ),legend=dict(\n",
    "#                 orientation=\"v\"))\n",
    "# py.iplot(dict(data=data, layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97c06a0e13facce9121fb56d5cf3dd117749e51a"
   },
   "source": [
    "We can see that number of films and total revenue are growing, which is to be expected. But there were some years in the past with a high number of successful films, which brought high revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "24403b85f8ccf956e4ca3de04dc2f39e193aa8cb"
   },
   "outputs": [],
   "source": [
    "# sns.catplot(x='release_date_weekday', y='revenue', data=train);\n",
    "# plt.title('Revenue on different days of week of release');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9ea182410c3df3ce322f8570b51a443261afb48"
   },
   "source": [
    "Surprisingly films releases on Wednesdays and on Thursdays tend to have a higher revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe0c60b0ed7f2083f3581c680d5d0416272093d8"
   },
   "source": [
    "<a id=\"runtime\"></a>\n",
    "### runtime\n",
    "\n",
    "The length of the film in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ae3587bf3c196d00292107146a5bfdf53fa3ef87"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 6))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.hist(train['runtime'].fillna(0) / 60, bins=40);\n",
    "# plt.title('Distribution of length of film in hours');\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.scatter(train['runtime'].fillna(0), train['revenue'])\n",
    "# plt.title('runtime vs revenue');\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.scatter(train['runtime'].fillna(0), train['popularity'])\n",
    "# plt.title('runtime vs popularity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e22f1900d813f4682f8e54df8d90c24997d4acff"
   },
   "source": [
    "<a id=\"tagline\"></a>\n",
    "### tagline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e40a80c45be1d769a6d863fbe74b1c19b4a6ac2c"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (12, 12))\n",
    "# text = ' '.join(train['tagline'].fillna('').values)\n",
    "# wordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.title('Top words in tagline')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cccf2021f08cd988fc45d6f1c7e6d74527d7b16"
   },
   "source": [
    "<a id=\"collections\"></a>\n",
    "### Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "e5b959dba27bbe74ab71ed21c18681bf1724b13a"
   },
   "outputs": [],
   "source": [
    "# sns.boxplot(x='has_collection', y='revenue', data=train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8639ee40558aa0bd99dc375391e8a83bb949cab"
   },
   "source": [
    "Films, which are part of a collection usually have higher revenues. I suppose such films have a bigger fan base thanks to previous films."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6c8dbb4e924247f1df9e25c938278584f078507"
   },
   "source": [
    "<a id=\"genres_\"></a>\n",
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "6d06937abeea5848adae30a31e49c6f711264ab4"
   },
   "outputs": [],
   "source": [
    "# sns.catplot(x='num_genres', y='revenue', data=train);\n",
    "# plt.title('Revenue for different number of genres in the film');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "5dc462e4dedb32a8a9558f6dd629619bed2d482b"
   },
   "outputs": [],
   "source": [
    "# sns.violinplot(x='genre_Drama', y='revenue', data=train[:100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "b3f63777808be2f4bf85dea68dd8b0f8f332a680"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(3, 5, figsize=(24, 12))\n",
    "# plt.suptitle('Violinplot of revenue vs genres')\n",
    "# for i, e in enumerate([col for col in train.columns if 'genre_' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9565a25d5842131215edce5833b68d752c89e251"
   },
   "source": [
    "Some genres tend to have less revenue, some tend to have higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92ebe983c07c8bd4627912b040cf611088ce9d16"
   },
   "source": [
    "<a id=\"prod_comp\"></a>\n",
    "### Production companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "7baa1946748738be98ae56ee5fabe4c400ace547"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(6, 5, figsize=(24, 32))\n",
    "# plt.suptitle('Violinplot of revenue vs production company')\n",
    "# for i, e in enumerate([col for col in train.columns if 'production_company' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "552b9de5d338a7ce0074da3438f06271981f144c"
   },
   "source": [
    "There are only a couple of companies, which have distinctly higher revenues compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92ebe983c07c8bd4627912b040cf611088ce9d16"
   },
   "source": [
    "<a id=\"prod_count\"></a>\n",
    "### Production countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "56d8e2dd2b3783824565cca15f3a1a25aa8651e4"
   },
   "outputs": [],
   "source": [
    "# sns.catplot(x='num_countries', y='revenue', data=train);\n",
    "# plt.title('Revenue for different number of countries producing the film');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55c88204a66b93493a5dfc835221a136869ec87e"
   },
   "source": [
    "In fact I think that number of production countries hardly matters. Most films are produced by 1-2 companies, so films with 1-2 companies have the highest revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "7baa1946748738be98ae56ee5fabe4c400ace547"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(5, 5, figsize=(24, 32))\n",
    "# plt.suptitle('Violinplot of revenue vs production country')\n",
    "# for i, e in enumerate([col for col in train.columns if 'production_country' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "552b9de5d338a7ce0074da3438f06271981f144c"
   },
   "source": [
    "There are only a couple of countries, which have distinctly higher revenues compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5896b7ff192a4f36146f7480e7b867863ebfff5"
   },
   "source": [
    "<a id=\"cast_viz\"></a>\n",
    "### Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "c06e28a47c6da9ec144e3515bc220b88830dc80c"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(train['num_cast'], train['revenue'])\n",
    "# plt.title('Number of cast members vs revenue');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(train['num_cast'], train['log_revenue'])\n",
    "# plt.title('Log Revenue vs number of cast members');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "d2507d0f239b00f2e50d629436561984d94a5752"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(3, 5, figsize=(24, 18))\n",
    "# plt.suptitle('Violinplot of revenue vs cast')\n",
    "# for i, e in enumerate([col for col in train.columns if 'cast_name' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "d56582228b5ff5dbfaead79b2053d67060a780ce"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(3, 5, figsize=(24, 18))\n",
    "# plt.suptitle('Violinplot of revenue vs cast')\n",
    "# for i, e in enumerate([col for col in train.columns if 'cast_character_' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12bad034af611557fe56ac269a86b696f0cdf427"
   },
   "source": [
    "<a id=\"key_viz\"></a>\n",
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "cca55dba5aad4640142b8748c15397bc41273e3e"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(6, 5, figsize=(24, 32))\n",
    "# plt.suptitle('Violinplot of revenue vs keyword')\n",
    "# for i, e in enumerate([col for col in train.columns if 'keyword_' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5af48ccbda6e9f9d02dd7a541f0c0dff40ec8216"
   },
   "source": [
    "<a id=\"crew_viz\"></a>\n",
    "### Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "55cfbd189e7068c731d07c70a7d6bd6bfb78292b"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(train['num_crew'], train['revenue'])\n",
    "# plt.title('Number of crew members vs revenue');\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(train['num_crew'], train['log_revenue'])\n",
    "# plt.title('Log Revenue vs number of crew members');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "dcadbb6e2565e834b1ba8a9bd37b02faed2d193c"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(3, 5, figsize=(24, 18))\n",
    "# plt.suptitle('Violinplot of revenue vs crew_character')\n",
    "# for i, e in enumerate([col for col in train.columns if 'crew_character_' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "5c58dc2c777078e4c52495e2b512bb31681e435b"
   },
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(3, 5, figsize=(24, 18))\n",
    "# plt.suptitle('Violinplot of revenue vs jobs')\n",
    "# for i, e in enumerate([col for col in train.columns if 'jobs_' in col]):\n",
    "#     sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ba0024111403d77d8ab177767e0ddd5a40455dd"
   },
   "source": [
    "<a id=\"basic_model\"></a>\n",
    "## Modelling and feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "a4ceed1b74dd3de3b7678afa5409fc408464d8f0"
   },
   "outputs": [],
   "source": [
    "for col in ['original_language', 'collection_name', 'all_genres']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n",
    "    train[col] = le.transform(train[col].fillna('').astype(str))\n",
    "    test[col] = le.transform(test[col].fillna('').astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_uuid": "6c0104e927d1ad9f0c23cc5d12d04b34c0893325"
   },
   "outputs": [],
   "source": [
    "train_texts = train[['title', 'tagline', 'overview', 'original_title']]\n",
    "test_texts = test[['title', 'tagline', 'overview', 'original_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "f05aa3c34b2b1d3ca528461e2c655545773b5245"
   },
   "outputs": [],
   "source": [
    "for col in ['title', 'tagline', 'overview', 'original_title']:\n",
    "    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n",
    "    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n",
    "    train = train.drop(col, axis=1)\n",
    "    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n",
    "    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n",
    "    test = test.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "55356a620f90a4475c41eaedbd95db6f83543725"
   },
   "outputs": [],
   "source": [
    "# data fixes from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n",
    "train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\n",
    "train.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \n",
    "train.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\n",
    "train.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\n",
    "train.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \n",
    "train.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\n",
    "train.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\n",
    "train.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\n",
    "train.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\n",
    "train.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\n",
    "train.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n",
    "train.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\n",
    "train.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\n",
    "train.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \n",
    "train.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \n",
    "train.loc[train['id'] == 1542,'budget'] = 1              # All at Once\n",
    "train.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\n",
    "train.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\n",
    "train.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\n",
    "train.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\n",
    "train.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n",
    "train.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\n",
    "train.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\n",
    "train.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\n",
    "train.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\n",
    "train.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\n",
    "train.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\n",
    "test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\n",
    "test.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\n",
    "test.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\n",
    "test.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\n",
    "test.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\n",
    "test.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\n",
    "test.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\n",
    "test.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\n",
    "test.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\n",
    "test.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n",
    "\n",
    "power_six = train.id[train.budget > 1000][train.revenue < 100]\n",
    "\n",
    "for k in power_six :\n",
    "    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_uuid": "4d4f4a5b8945723acb5a1206cffc60fbab151ae8"
   },
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'revenue'], axis=1)\n",
    "y = np.log1p(train['revenue'])\n",
    "# y = train['revenue']\n",
    "X_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_uuid": "dd747f7dc0506664d8dbe8e3eb4dc8b6c71f244a"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "994c032c62219dceec9c287f83ac91aeff8c8239"
   },
   "outputs": [],
   "source": [
    "# params = {'num_leaves': 30,\n",
    "#          'min_data_in_leaf': 20,\n",
    "#          'objective': 'regression',\n",
    "#          'max_depth': 5,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.2,\n",
    "#          \"verbosity\": -1}\n",
    "\n",
    "# model1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "# model1.fit(X_train, y_train, \n",
    "#         eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "#         verbose=1000, early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_uuid": "f0eb8c69f9ffd8d0e664ec331e001ab59552d5ff"
   },
   "outputs": [],
   "source": [
    "# eli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "_uuid": "c5d7d29134a25355c517250b00079dae64523f5e"
   },
   "outputs": [],
   "source": [
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18146005, 0.9415344 , 0.50298877],\n",
       "       [0.79408627, 0.48528662, 0.35034718],\n",
       "       [0.8911608 , 0.90323986, 0.71356916],\n",
       "       [0.1983269 , 0.00744653, 0.90131838],\n",
       "       [0.56249088, 0.90128987, 0.16638828],\n",
       "       [0.70645768, 0.99443143, 0.79233542]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "n = random.randint(1, 10)\n",
    "\n",
    "a = np.random.rand(2,3, n) # 2 * 3 * n = 24\n",
    "# np.ndarray\n",
    "# a.reshape(12, 2) # 12 * 2 = 24\n",
    "# n\n",
    "print(n)\n",
    "a.reshape(6, -1) #  n = 2 * k -> k = n / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ff384168227fc22728aea371bb5935d0f39ce6eb"
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X, \n",
    "    X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "    \n",
    "    # out of fold - 1000\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    # \n",
    "    prediction = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "#         if model_type == 'sklearn':\n",
    "#             X_train, X_valid = X[train_index], X[valid_index]\n",
    "#         else:\n",
    "        X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "            \n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "    \n",
    "        if model_type == 'sklearn':\n",
    "            # Callable объект .fit\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_squared_error(y_valid, y_pred_valid)\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,) # Shape [1, 200] -> [200]  -1  всё остальное\n",
    "        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n",
    "        \n",
    "        prediction += y_pred    \n",
    "        \n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Oct 24 19:05:49 2022\n",
      "2700 300\n"
     ]
    }
   ],
   "source": [
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.values[train_index], X.values[valid_index]\n",
    "    print(len(X_train), len(X_valid))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_train).sum() # False -> 0 True -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Mon Oct 24 19:04:39 2022\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-965260d88a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m oof_linear, prediction_linear, _ = train_model(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sklearn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-663f9279692a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, X_test, y, params, folds, model_type, plot_feature_importance, model)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Callable объект .fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    519\u001b[0m                                    y_numeric=True, multi_output=True)\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    721\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 10,\n",
    "         'objective': 'regression',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "\n",
    "oof_linear, prediction_linear, _ = train_model(\n",
    "    X, X_test, y, params=params, model_type='sklearn',\n",
    "    model=LinearRegression(),\n",
    "    plot_feature_importance=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_uuid": "1320338669af3caa99ef85d4ab8e24d2b5a6e31f"
   },
   "outputs": [],
   "source": [
    "# params = {'num_leaves': 30,\n",
    "#          'min_data_in_leaf': 10,\n",
    "#          'objective': 'regression',\n",
    "#          'max_depth': 5,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.2,\n",
    "#          \"verbosity\": -1}\n",
    "# oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d7485f6b856a3d0535f56c269ef3a34bf4e0d58"
   },
   "source": [
    "<a id=\"oof\"></a>\n",
    "### OOF features based on texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "_uuid": "1ae6d67d8486234661379602e8ad43dd3bc1edff"
   },
   "outputs": [],
   "source": [
    "# for col in train_texts.columns:\n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#                 sublinear_tf=True,\n",
    "#                 analyzer='word',\n",
    "#                 token_pattern=r'\\w{1,}',\n",
    "#                 ngram_range=(1, 2),\n",
    "#                 min_df=10\n",
    "#     )\n",
    "#     vectorizer.fit(list(train_texts[col].fillna('')) + list(test_texts[col].fillna('')))\n",
    "#     train_col_text = vectorizer.transform(train_texts[col].fillna(''))\n",
    "#     test_col_text = vectorizer.transform(test_texts[col].fillna(''))\n",
    "#     model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n",
    "#     oof_text, prediction_text = train_model(train_col_text, test_col_text, y, params=None, model_type='sklearn', model=model)\n",
    "    \n",
    "#     X[col + '_oof'] = oof_text\n",
    "#     X_test[col + '_oof'] = prediction_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7e3f87bf30dbd425de5909fa2a8de6ee3cb6246"
   },
   "source": [
    "<a id=\"add_feat\"></a>\n",
    "### Additional feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33a897ca5503e3bb4b6ab28e89a4056644f116e1"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "526607c3368a9128f137931b68c59d0923006b1e"
   },
   "outputs": [],
   "source": [
    "def new_features(df):\n",
    "    df['budget_to_popularity'] = df['budget'] / df['popularity']\n",
    "    df['budget_to_runtime'] = df['budget'] / df['runtime']\n",
    "    \n",
    "    # some features from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n",
    "    df['_budget_year_ratio'] = df['budget'] / (df['release_date_year'] * df['release_date_year'])\n",
    "    df['_releaseYear_popularity_ratio'] = df['release_date_year'] / df['popularity']\n",
    "    df['_releaseYear_popularity_ratio2'] = df['popularity'] / df['release_date_year']\n",
    "    \n",
    "    df['runtime_to_mean_year'] = df['runtime'] / df.groupby(\"release_date_year\")[\"runtime\"].transform('mean')\n",
    "    df['popularity_to_mean_year'] = df['popularity'] / df.groupby(\"release_date_year\")[\"popularity\"].transform('mean')\n",
    "    df['budget_to_mean_year'] = df['budget'] / df.groupby(\"release_date_year\")[\"budget\"].transform('mean')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe3bc388686ec280b6a8b0f59a50afe009623d95"
   },
   "outputs": [],
   "source": [
    "X = new_features(X)\n",
    "X_test = new_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc6899e5988a42a0c64a29385868abe0510b9632"
   },
   "outputs": [],
   "source": [
    "oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6684ca81fe35bc8a1d12622b2f6686cf01d004bd"
   },
   "source": [
    "<a id=\"imp_feats\"></a>\n",
    "### Important features\n",
    "\n",
    "Let's have a look at important features using ELI5 and SHAP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff82b9193ff22a66f2d869ea11ad2d041c785084"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 6,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "model1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "model1.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "        verbose=1000, early_stopping_rounds=200)\n",
    "\n",
    "eli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6c8c94a775c4d7215bd442dfeeb8571ad464c67"
   },
   "source": [
    "We can see that important features native to LGB and top features in ELI5 are mostly similar. This means that our model is quite good at working with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ed18e95e2da01e8db6bbd9e6694626f5af46d8f"
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model1, X_train)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f09d214bb08b6f4e9ba4454c7a25b6450493fc1a"
   },
   "source": [
    "SHAP provides more detailed information even if it may be more difficult to understand.\n",
    "\n",
    "For example low budget has negavite impact on revenue, while high values usually tend to have higher revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb1a6744d4228f438d6c868ae667e3ee6a62ee97"
   },
   "outputs": [],
   "source": [
    "top_cols = X_train.columns[np.argsort(shap_values.std(0))[::-1]][:10]\n",
    "for col in top_cols:\n",
    "    shap.dependence_plot(col, shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c712b24954c7b0b86b16dfb91506b5b78f8db1b0"
   },
   "source": [
    "Here we can see interactions between important features. There are some interesting things here. For example relationship between release_date_year and log_budget. Up to ~1990 low budget films brought higher revenues, but after 2000 year high budgets tended to be correlated with higher revenues. And in genereal the effect of budget diminished.\n",
    "\n",
    "Let's create new features as interactions between top important features. Some of them make little sense, but maybe they could improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7aa37af7272d4403f238b78eed028d4f558fc806"
   },
   "outputs": [],
   "source": [
    "def top_cols_interaction(df):\n",
    "    df['budget_to_year'] = df['budget'] / df['release_date_year']\n",
    "    df['budget_to_mean_year_to_year'] = df['budget_to_mean_year'] / df['release_date_year']\n",
    "    df['popularity_to_mean_year_to_log_budget'] = df['popularity_to_mean_year'] / df['log_budget']\n",
    "    df['year_to_log_budget'] = df['release_date_year'] / df['log_budget']\n",
    "    df['budget_to_runtime_to_year'] = df['budget_to_runtime'] / df['release_date_year']\n",
    "    df['genders_1_cast_to_log_budget'] = df['genders_1_cast'] / df['log_budget']\n",
    "    df['all_genres_to_popularity_to_mean_year'] = df['all_genres'] / df['popularity_to_mean_year']\n",
    "    df['genders_2_crew_to_budget_to_mean_year'] = df['genders_2_crew'] / df['budget_to_mean_year']\n",
    "    df['overview_oof_to_genders_2_crew'] = df['overview_oof'] / df['genders_2_crew']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1dcc3499f4cfbc90d1a896011546b562d0adee75"
   },
   "outputs": [],
   "source": [
    "X = top_cols_interaction(X)\n",
    "X_test = top_cols_interaction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "381d4d7c8f7e439511ce3f7b84014dd417aaa57d"
   },
   "outputs": [],
   "source": [
    "X = X.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], 0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04073ee9ec7ce9667c8e93205c02bf85a6151b9c"
   },
   "source": [
    "<a id=\"ext_feats\"></a>\n",
    "### External features\n",
    "I'm adding external features from this kernel: https://www.kaggle.com/kamalchhirang/eda-feature-engineering-lgb-xgb-cat by kamalchhirang. All credit for these features goes to him and his kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9b40fa7274e33bef1318a5519b386586a120d0b"
   },
   "outputs": [],
   "source": [
    "trainAdditionalFeatures = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\n",
    "testAdditionalFeatures = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\n",
    "\n",
    "train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n",
    "test = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\n",
    "X['imdb_id'] = train['imdb_id']\n",
    "X_test['imdb_id'] = test['imdb_id']\n",
    "del train, test\n",
    "\n",
    "X = pd.merge(X, trainAdditionalFeatures, how='left', on=['imdb_id'])\n",
    "X_test = pd.merge(X_test, testAdditionalFeatures, how='left', on=['imdb_id'])\n",
    "\n",
    "X = X.drop(['imdb_id'], axis=1)\n",
    "X_test = X_test.drop(['imdb_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27f2315c67c8065f388f0955a1987637f4ff888a"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0125f421a8cc732156c60d8a1e13efbcbcc3d577"
   },
   "outputs": [],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 9,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e4232d7e976c7782d911f273fe6d0499b53337f"
   },
   "source": [
    "<a id=\"blending\"></a>\n",
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "723ee5fb3f48acacd3d0c81b18eed95122da3291"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b33588be4174ea5656d55cbdc4eb1e11d625e768"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81826fc9db3782b67513c57265d22788e1515826"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f4dda96c450fe41c853c6d2c482d525e586e293"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1658d1b96c2ee02789f822f31a25425b727b0ef2"
   },
   "source": [
    "<a id=\"stacking\"></a>\n",
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f16996349c42b28229b13f33b974755b6202baf"
   },
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof_lgb, oof_xgb, oof_cat, oof_lgb_1, oof_lgb_2]).transpose()\n",
    "train_stack = pd.DataFrame(train_stack, columns=['lgb', 'xgb', 'cat', 'lgb_1', 'lgb_2'])\n",
    "test_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_cat, prediction_lgb_1, prediction_lgb_2]).transpose()\n",
    "test_stack = pd.DataFrame(test_stack, columns=['lgb', 'xgb', 'cat', 'lgb_1', 'lgb_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b93fb33cd8d417349678db6f2def72e2df6cb35c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91c27020935e11d96b41e395a535eb86a65837d2"
   },
   "outputs": [],
   "source": [
    "model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n",
    "oof_rcv_stack, prediction_rcv_stack = train_model(train_stack.values, test_stack.values, y, params=None, model_type='sklearn', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29fddbb3e3e22345439c0edcf5ee53fd877168f6"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')\n",
    "sub['revenue'] = np.expm1(prediction_lgb)\n",
    "sub.to_csv(\"lgb.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) / 2)\n",
    "sub.to_csv(\"blend.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat) / 3)\n",
    "sub.to_csv(\"blend1.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat + prediction_lgb_1) / 4)\n",
    "sub.to_csv(\"blend2.csv\", index=False)\n",
    "sub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat + prediction_lgb_1 + prediction_lgb_2) / 5)\n",
    "sub.to_csv(\"blend3.csv\", index=False)\n",
    "\n",
    "sub['revenue'] = prediction_lgb_stack\n",
    "sub.to_csv(\"stack_lgb.csv\", index=False)\n",
    "sub['revenue'] = prediction_rcv_stack\n",
    "sub.to_csv(\"stack_rcv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "723d4b7bc280cd31fdada53ad6420192b9a3a8d60631096143cc718cb9440dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
